######################################################################
# OLLAMA LLM CONFIGURATION EXAMPLE
# Copy this section to your .env file to use Ollama for AI messages
######################################################################

# Enable AI message generation
LLM_ENABLE=True

# Use Ollama provider for local LLM
LLM_PROVIDER=ollama

# Ollama server configuration
# Replace with your Ollama server IP address
LLM_OLLAMA_HOST=http://192.168.1.100
LLM_OLLAMA_PORT=11434

# Model to use (default: gemma2:2b)
# Popular options:
# - gemma2:2b     (Recommended - 4B, fast, good quality)
# - llama3.2:3b   (Fast, good quality)
# - qwen2.5:3b    (Great for technical content)
# - mistral:7b    (Higher quality, slower)
# - phi3:3b       (Very fast, Microsoft)
LLM_MODEL=gemma2:2b

# Optional: Retry configuration
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY_BASE=2

######################################################################
# SETUP INSTRUCTIONS:
#
# 1. Install Ollama on your server:
#    curl -fsSL https://ollama.com/install.sh | sh
#    
#    Multi-GPU setup? See: https://github.com/ChiefGyk3D/FrankenLLM
#
# 2. Discover available models:
#    Browse: https://ollama.com/library
#    List local: ollama list
#    Search: ollama search gemma
#
# 3. Pull the model:
#    ollama pull gemma3:4b
#
# 4. Start Ollama (if not running as service):
#    OLLAMA_HOST=0.0.0.0 ollama serve
#
# 5. Update LLM_OLLAMA_HOST above with your server IP
#
# 6. Test connection:
#    curl http://192.168.1.100:11434/api/tags
#
# NOTE: You can have both Gemini and Ollama configs in the same .env!
#       Just add GEMINI_API_KEY=... below and switch providers with:
#       LLM_PROVIDER=gemini  or  LLM_PROVIDER=ollama
#
######################################################################
