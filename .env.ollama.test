# Test Configuration for Ollama
# Copy these lines to your .env file or use: source .env.ollama.test

# Enable AI
LLM_ENABLE=True

# Use Ollama provider
LLM_PROVIDER=ollama

# Your Ollama server details (CHANGE THESE)
LLM_OLLAMA_HOST=http://192.168.1.100  # Replace with your server IP
LLM_OLLAMA_PORT=11434

# Model to use (make sure this model is installed on your server)
LLM_MODEL=gemma2:2b

# Test settings
LLM_MAX_RETRIES=3
LLM_RETRY_DELAY_BASE=2
